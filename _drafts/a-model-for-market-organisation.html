---
layout: post
title: A model for market organisation
date:
type: post
parent_id: '0'
published: false
password: ''
status: draft
categories: []
tags: []
meta:
  _wpcom_is_markdown: '1'
author:
  login: pboes
  email: paul.boes.10@ucl.ac.uk
  display_name: pboes
  first_name: ''
  last_name: ''
permalink: "/"
---
<p>I recently read Moses Naim's book "The end of power", which had been the first book of Marc Zuckerberg's "Year of Books". The basic argument of the book, as far as I see it, is that power, the capacity of one party to enforce/induce behaviour on other parties, is more and more difficult to maintain by any single party. The reason he sees for this is mostly that entry barriers to power, the kinds of barriers that made access to power exclusive for such a long time (descent, education, capital - not in that order...) have been decreasing, opening up competition for power positions and thus naturally leading to a less stable power dynamics (if it is easy for many people to bring themselves into power positions, then necessarily it will also be difficult for people in power positions to stay there). Naim goes on to argue this point in several areas - national and global politics, religion, economics, trade unions. In the case of economics he presents Weber's account of the advantages of bureaucratization in the industrial age: “Where the bureaucratization of administration has been completely carried through, a form of power relation is established that is</p>
<p>practically unshatterable.” (quoted from Weber's "Economics and Society", p.45). The way in which this links back to Naim's argument is through the seminal work of Ronald Coase in his "The Nature of the Firm", in which transaction costs are introduced (as well as the famous Coase Theorem dealing with the efficient allocation of externalities, which however has very little to do with what I'm talking about here). In short, with transaction costs (contracting costs, administrative costs, communication costs, etc) taking up a considerable share in the overall costs, the ability to avoid them constituted an entry barrier in the above sense: Companies with the means to avoid transaction costs could produce larger margins and, more importantly, such an initial advantage could be used by those companies to further stabilise their position: The high transaction costs here appear as the explanation for the emergence of economics of scale, of scope, etc. Naim then goes on to give many examples where nowadays the lower transaction costs mean that Weber's statement does not apply anymore.</p>
<p>So far, so good. However, I felt that Naim's argument left some obvious questions unanswered:</p>
<ol>
<li>Only because high transaction costs favour vertical integration, this doesn't mean that low transaction costs favour the opposite. There had to be more to explaining the transition from hierarchical to non-hierarchical (or less hierarchical) organisation of economical, political or military structures.</li>
<li>
<p>What was not discussed at all in the book was the apparent snowball effect: It's not as if we see a turn away from forms of vertical integration only in certain markets. Different kinds of fragmentisation, albeit admittedly of varying flavour, appear seemingly everywhere: From empire to state, from General Motor to Start-Ups,  from in-house-employee to contractor, from army to warlords, from coal to air-water-solar-mixtures...How to explain the dynamics that induce the above shift across all these levels (assuming that this is not merely a coincidence, which I hope you're willing to assume with me)?</p>
</li>
</ol>
<p>In a way the second, inter-level, question is really the more interesting but also the more complicated and an answer to the first, intra-level, question certainly necessary to tackle the second.</p>
<p>So I made answering these questions a little exercise for my time in the train to the office and the following simple model is the outcome: The brief answer to the question: 1. The determining factor for intra-structural organisation is a tradeoff between transaction costs and the mutual information (i.e. the degree of correlation) between the input this level has to process, rather than the costs alone; 2. The snowball effect is best understood as reflecting the Data Processing Inequality, together with some modest assumption on the order of magnitude of the transaction costs at subsequent levels (and their finitude).</p>
<p>The rest will simply discuss a simple model that brings me to those answers.</p>
<p><b>The Model</b></p>
<p>Think of a company as consisting of n employees that play the game "being a company" according to the following rules: k employees, so called "receptors" are special in that at the beginning of each round they receive information in the form of the value of one bit, so either "0" or "1". So if we assign an index to the "receptors", calling them n_1, n_2, ...,n_k, we can represent the total input information as a string of k bits "010....10" with the first bit being the digit n_1 received and so on. Let's call this string S and denote its ith digit by S_i. The goal of the each employee is to guess S. The players can distribute the information between them by installing links that connect them pairwise. So, if two employees share a link and one of the knows something about the input bit of "receptor" n_i (i.e. he knows S_i), then by communicating over this link he can tell S_i to the other one, which then will also know it and can communicate it further to his neighbours (the "neighbours" of an employee are all those employees with which he shares a link). However, firstly the installation of every link will cost the company some amount t. These are just the transaction costs that accompany links. Moreover, since time is short and money, in any given round the information can only propagate along the links in some fixed number l of steps. So if, for example, l=2, then at the end of the round all the employees that are two links away from receptor n_3 will know S_3 and nobody else, and of course the same for all other receptors. As the last, and most important, ingredient we say that there is a probability with which each string S occurs: That is, if SS is the set of aaaaallll possible ways of combining k bits (there are exactly 2^k), then there is some function P that assigns to every k-bit string in SS a probability of being the input string in such a way that all the probabilities sum up to 1. We also assume that every employee knows P, that is, he or she knows how likely a string is to occur (I'll comment on the meaning of this assumption a bit later).</p>
<p>Let's illustrate all of this to make sure we're on the same page:</p>
<p>Consider a company consisting of three employees - n_1,n_2,n_3 -, in which two - n_1 and n_2 -  are receptors. Let's also say that there are two links in this company, one connecting n_1 and n_3 as well as one connecting n_2 and n_3. If you picture this, this will just give the simplest non-trivial perfect tree graph, i.e. a tree of just one node from which two nodes exit.</p>
<p>In this company, SS = {00,01,10,11} and if we set l=0, then we have that n_1 knows S_1, n_2 knows S_2 and S_3 doesn't know anything about the actual value of S. While if we set l=1, then n_1 knows S_1, n_2 knows S_2 and n_3 knows the complete string (since he is one link away from all receptors). Finally, if we set l=2 then all three employees know the whole string.</p>
<p>If you're with me so far, then everything we're all ready to go. Let's first consider the first question, that is, how can we properly understand the decision of the company to organise vertically or non-vertically. The aim of the company is to maximize its profit. The profit is given by the revenue minus the cost. So to make our lifes a lives a little simpler lets construe this question as follows: Let's say that the company only has a choice between two possible ways of organizing itself: one where it's protoypically hierarchical and one where it's prototypically non-hierarchical. The first will be a perfect binary tree (LINKTHIS!!!), while the other will be a company in which every employee shares a fixed amount of links and the links are completely symmetrically distributed, so there's no boss or natural "top-node" like in the case of the tree. The question then is: Under what circumstances is it profitable to switch from one mode of organisation to another. Well, since we made our lives very simple, this is not a difficult question: The costs for the company, in this simple money are made up of the loan for the employees and the total cost for the links being installed. Moreover, since the loan cost depends only on the number of employees (we don't pay distinguish between employee roles here), it will be the same for both modes of organisation and not matter for answering the question, so we'll ignore it. Thus, the only cost left is t*#links, where t is still the transaction cost per link. Let's move on to the revenue: Here things get more interesting. As I said, the aim of each employee is to guess to input string S (this is just modelling the general truth that the performance of a company is its ability to react upon input information). So let's say that if an employee guesses S right, the revenue of the company increases by some amount r. The chance that any employee will guess the string right, though, depends of course massively on the information he has about the string.</p>
<p>For example, if an employee has no information about the actual input string, then all he/she has is his/her knowledge of the general probability distribution over SS. Here, the concept of Shannon entropy H(LINK THIS) comes in very handy: It's the natural measure of the "information content" of a probability distribution. In our case, it takes the probability distribution P and returns a number between 0 and k that quantifies how much information is contained in P: If H(S)=0, then an employee has no uncertainty about what S will be even before he/she is told about it and so he/she can learn nothing new by being told its value - in this case P has no information content for him/her. If, however, H(S)=k, then this means that he/she is completely uncertain about what S will be and so will learn as much as he/she could possibly learn by being told the value of its value - in this case P has high information content for him/her. At the same time H(S) quantifies the uncertainty about the actual value of S at the beginning of the round, before anything is communicated. Since the more certain you are about what the string will be, the more likely you are to guess it correct, the <em>average</em> contribution of an employee that has no information about the actual string is given by (k-H(S))/k*r. So if the employee there is complete certainty about what the string will be (H(S)=0), then the contribution will be r, while if there is complete uncertainty about what it will be, the contribution will be 0.</p>
<p>What about the case in which some employee did actually learn some of the values of S during the round? Then his/her uncertainty will of course be reduced. The relevant quantity here is the <em>relative</em> Shannon entropy H(S|S_i,S_j,...) - the uncertainty about S <em>given</em> that you know the bits S_i,S_j,...Thus, if we denote by B_i = {S_j,S_k,...} the set of all bits that employee i learned about by the end of each round, then the overall average profit of the company for a given mode of organisation o, ignoring loans, will be:</p>
<p>\Pi^o = \Bigsum_i (k - H(S|B_i^o))/k<em>r - t</em>#links^o.</p>
<p>So the costs are simply the transaction cost times the number of links in that mode of organisation, while the revenue is the statistical likelihood of the employee guessing the string right, i.e. the information he has at the end of the round - (k - H(S|B_i^o))/k -, times the profit for getting it right (r). Now, given the choice between any two modes of organisation o_1 and o_2, a company will choose o_1 over o_2, whenever \Pi^{o_1} \geq \Pi^{o_2}$ (if the profits are the same, then it is indifferent between them). Substituting the above euqation into this, one sees that this is the case when</p>
<p>\be</p>
<p>\frac{H^{(1)}(S)-H^{(2)}(S)}{#links_2-#links1} \geq \frac{t}{r},</p>
<p>\ee</p>
<p>where $H^{(o)}(S):= \frac{1}{k}\bigsum_i^n H(S|B_i^o)$ is simply the overall remaining uncertainty in the company at the end of a round. So this states just what one would expect: A company will switch between two modes of organisation if its improved performance due to the better knowledge of its employees outweighs any additional costs caused by having to install new links (of course, companies may improve their performance by merely redistributing the links, without having to install anything new. They will always choose to do so. Note also that in my model reducing the number of links can never have an advantage, since this can never increase the performance but at best keep it constant. So my model is "kackophony-insensitive").</p>
<p>Even though I really don't want to go into much detail about entropies, it is essential to at least distinguish between two different determinants of the entropy: Say k=2, so there are only two input bits and  SS, the space of strings, is simply SS = {00,01,10,11}. In principle the entropy H(S) for such a string lies in the range between 0 and 2. Now consider two diferent probability distributions P over SS: the first assigns the probabilities {0,0,0.5.,0.5} to the elements of SS, while the second assigns the values {0.5,0,0,0.5}. The first just says that we know the value of S_1 for sure, it will be 1, but we have no clue about the value of the second bit, both "10" and "11" appear with equal likelihood. The second says that the two bits are definitely going to be the same digit, so it will either be 00 or 11, but we are completely uncertain about which of the two it will be. These two probability distribution yield the same entropy, H(S) = 1 (so even though there are two bits, the actual value of the string can be communicated via a single bit in either scenario). But they do so for very different reasons: In the first scenario, we really only dealt with one bit, because the first was already fixed to be 1. In the second scenario, both bits genuinely mattered for the uncertainty but it was because of the <em>correlation</em> between them ("tell me about one, and I can tell you about the other") that the entropy was lower than the maximum. So entropy can be lower than its maximum value for two fundamentally different reasons: because of the existence of statistical correlations between inputs or simply because for some of the inputs one outcome is much more likely than the other. In practice, of course both will play a part but the take-home message of this paragraph is that.</p>
<p>This is important because communicating values of the S_i across the company can only lower the uncertainty due to correlations (IS THAT EVEN CORRECT?). results across the company can only increase performance H(S|B_i) &lt; H(S) only if</p>
<p>But actually this goes already beyond the simple Coase story: The transaction costs t are only one factor, what matters is also the degree of correlations at the input. If H(S) is very low, that is, if knowing no bit or only very few bits of S pretty much gives you full knowledge of the rest of S, then adding links will usually be a waste of resources. With respect to Weber's quote, for example, we now recognize that vertical integration and bureaucratization were mainly so powerful because there were basically no surprises in the industry: Resources and labour were abundant, the mission was clear, etc. This lack of uncertainty implied that people working in one branch of the company, or sitting at some point in the supply chain, could most of the time do just the right thing for the company even though they had no clue what was actually going on in other parts of the latter - simply by following the strategy "assume that nothing has changed in other parts of the company and act accordingly". Such strategies are of course exactly what I am modelling in my game by making people guess the <em>whole</em> string S - in principle, what is the best thing to do for an employee in the company IS a matter of what happens in the rest of the company. But since most of the times companies operate in stable environments in which the above strategy works, the employees do a very good guessing job of the whole string by <em>implicitly</em> make a guess on what goes on elsewhere and explicitly react only on their local information. However, once you take away this stability (as we will in a minute), these things start to matter more apparently. Note also that I would justify my assumption that every employee has access to the full probability distribution P similarly: Working instructions and routines, best practices, etc., are an implicit way of communicating expected input probabilities. They are the result of having learned from previous inputs. So my assumption of knowledge of P is simply how I model this fact in my guessing game.</p>
<p>Let's turn to the second question. So far we have learned that the decision of a company, at least those that can be modelled by my game, which mode of organisation to install, depends on a tradeoff between the increase of performance due to internal communication and increase of cost due to installing the infrastructure necessary for this communication. Why should any company's decision, or any entity's decision more generally, about this have an effect on other companies or at different scales? Here my model will slowly break down under the weight of additional assumptions that I have to make but it still captures the essential point I think.</p>
<p>Note first that under any mode of organisation H^{(o)}(S), which always lies between 0 and n, will never decrease as one raises H(S): By making the raw input uncertainty larger (and keeping everything else fixed) the uncertainty at the end of the round cannot decrease, no matter how clever the links are distributed.</p>
<p>Now consider the following: There is a second company B that wants to guess the output of our company A here. Maybe they're a competitor, maybe what we model is also simply a company's price politics and so "guessing a company's output" really means estimating the price for which it will sell its products...who knows, it doesn't really matter. When producing this guess, I assume that the B actually has complete information about the mode of organisation of A, and they also know P (and therefore H(S)).</p>
<p>Since it was the aim of the first company to guess S, let's say that the output of A is simply the average guess taken over all employees of A: For each employee the probability of guessing some string S' at the end of the round is given by P conditioned on the information he got during the round. Let's call this probability distribution of employee i at the end of the round P_i. The overall probability P' of the company's output being S' is then given by P'(S') =\frac{1}{n}\bigsum_i P_i(S'), and the same for all other strings. FALSE::: The uncertainty of B, if B knows S, about the output of A is then H(S'), which is just given by P'. But, crucially, B doesn't know S, but only P. So the overall uncertainty HH(S) about the output of A is really H(S') + H(S), the uncertainty about S together with the uncertainty about the output once S has been fixed. But since H(S') is always positive, to be specific in the range 0 \leq H(S') \leq k, just like H(S), this trivially implies that H_B(S) is <em>lower bounded</em> by H_A(S)!! That is, from the point of view  B, the processing of the initial H(S) by A can only increase the uncertainty B itself faces (well, it could leave it unchanged in case the S was perfectly communicated in A but that's only a very special and unrealistic case. So this means that if there is a source process with some fixed uncertainty H(S), then the farther away from the processing chain a company sits, the more uncertain it will have to be about what ends up being its own input. THis is also pretty intuitive but it has, I think some big consequences for our question: (IS THIS REALLY TEH DATA PROCESSING INEQUALITY THOUGH?? Yes, the output entropy correlations between the source and B cannot be higher that those between source and A)</p>
<p>The entropy probability over the output of A is H_A(S) + H(S).</p>
<p>Now let's model a market and let's just assume that it has exactly that it's just N companies with K "receptor companies" that receive some exogenous input. Each of these companies, in turn, is just a copy of A, that is, there are n employees and k receptor employees. This simplifying assumption means that we can directly compare modes of organisation at the market level with that at the company level. We can then ask the following question: Given two modes of organisation o_1 and o-2, is there a relation between the point at which the market decides to switch between them and that the companies decide to switch between them?</p>
<p>But first I need to address one worry some reader may have: Markets and companies are completely different. To begin with, the participants in the one case are colleagues and interested in collaboration (at least in theory) and in the other their are competitors....I think this point is not problematic, once one really stresses that links between companies merely model the exchange of information between them, not more, not less. So if we imagine a market in which some players are competitors and others are not (say the "market" of all companies involved in producing toothbrushes). So links do not indicate "supplying contract" or "company merging" or anything like that. And information exchange can happen between rivalling companies (for example through external balances  and so on) but also between collaborating companies. So there's no problem with modelling parties that really stand in very different relationships as all the same in my model.</p>
<p>The second important point to make is: If we formulate the profit of the market just as we did in the case of the company, as</p>
<p>\be</p>
<p>\PI = ...</p>
<p>\ee</p>
<p>what does this mean? It means here that we assume that markets of any kind produce some good, possibly not material, that the more informed participants in the market are better at producing this good, that getting informed about this goal requires resources the spending of which lowers your ability to produce that good and that markets organise themselves in such a way that they maximise this good. I find it hard to think of anything worth calling a market where no such good exists. It may be the profit in the case of a company, it may be a good and cheap toothbrush, it may be fighting world hunger.</p>
<p>With these two things said, let's turn back to the question.</p>
<p>PICTURE.</p>
<p>In order to answer our question, we need first to relate the two levels, the company and the market level. Since each of the companies is a copy of A, this means that each of the K receptor companies will receive some string S^i at the beginning of a round, where S^i is an element of SS. So the overall input at the market level - call it T - must be the set of all input strings S^i, that is. T = {S^1, S^2, ..., S^K}. The space of all possible T is then the space over possible input strings. We call this space TT, which is just analogous to SS in the company case. The entropy of the market input string is then H(T), which is calculated from the probability distribution over TT. THe important link between the two levels is that we require that H(T) is such that it reduces locally to H(S). This just means for each receptor company i, $P(S^i)  = \bigsum_j P(T_i=S^i)$, i.e. the probability of company i receiving the string S^i in a round must be equal the total probability of the whole market receiving an input T during which company i receives S^i. This is not an assumption but required by consistency and obvious if you think about it.</p>
<p>But this link between H(T) and H(S) also implies that they move together.</p>
<p>One simple way to construct a T that satisfies this is by picking each S^i independently and according to H(S). Then the probability of some T={S^1,S^2,...S^K} occurring is just P(T) = P(S^1)\cdot P(S^2)....\cdotP(S^K) and in this case H(T) = K\cdot H(S). So if we assume that t*=Kt and k=Kt and !</p>
<p>But this means that exactly at the point at which each of the receptor companies decides to switch their mode of organisation, the whole market also decides to switch!!</p>
<p>Moreover, as we have learned before about the DPI, companies in the market further up will always be even more uncertain about H(T) and so be more likely to switch earlier!</p>
<p>This doesn't in fact mean that  So let's assume for the sake of the argument that there is in fact a critical value of H(S) at which $\displaystyle \frac{H^{(1)}(S)-H^{(2)}(S)}{#links_2-#links1} = \frac{t}{r}$. As the input entropy passes this point, the company will switch from mode o_2 to mode o_1.</p>
<p>Talk about correlations: If there is large degree of correlation between the single inputs then this will result in an overall very low</p>
<p>so things in these models shouldn't be taken to literally.</p>
<p>Consider another company receiving the output of our company as their input. (initially I tried to set up some "phase transtion of all companies by where they cyclically look at another but then this didn't work precisely because of the point that I will make below...)</p>
<p>Their uncertainty is lower bounded by H_J!!</p>
<p>And this is true even though from the point of the first company they reduce the uncertainty (in fact, for them it is upper bounded by k).</p>
<p>Also taking only boolean functions (say, the parity of the output). Assume: The cost of implementing links depends linearly on the size of the communication communicated through them. This may be motivated through cost, but also through, say, importance of noth having afulay messages (although we didn't say anything about epsilons in our model). In this case, the lower bound induces a snowball effect: This means that the critical threshold for the case of the</p>
<p>Let's assume that there is always some critical threshold: The cost of implementing additional links are never so high.</p>
<p>THen, the higher the lower bound on the input uncertainty, the less "space" the critical threshold has to be in (although there could still be a lot of space). As we shift up at the bottom, hte higher we go, the more likely it is that it is worth changing (really?)</p>
<p>There is the DPI between any two nodes and there's the question of the snowball effect which is intermarket. But the whole point is that it is impossible</p>
<p>There are two things: There is the fact that increasing H(S) and H(P) move up together. This is not a causal model, but it means that in</p>
